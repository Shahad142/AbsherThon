# -*- coding: utf-8 -*-
"""Rafad LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TP2HFi06q5h4DKxKHtwRowU7vNUxC0yl
"""

!pip install langchain_community

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
# !pip install langchain faiss-cpu sentence-transformers
# !pip install transformers accelerate bitsandbytes
#

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",  # Instruct fine-tuned model
    max_seq_length = 4096, # Increased max_seq_length to accommodate longer inputs
    dtype = None,
    load_in_4bit = True,
)

# ÿ™ÿ¨ŸáŸäÿ≤ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÑŸÑÿ™ŸàŸÑŸäÿØ
FastLanguageModel.for_inference(model)

from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

# ÿ•ÿπÿØÿßÿØ ÿßŸÑŸÄ pipeline ŸÖÿπ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨
qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=False,  # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ Greedy decoding
    return_full_text=False # ÿ•ÿ∂ÿßŸÅÿ© Ÿáÿ∞ÿß ŸÑÿ∂ŸÖÿßŸÜ ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÜÿµ ÿßŸÑÿ¨ÿØŸäÿØ ŸÅŸÇÿ∑
)

# ŸÑŸÅŸá ÿ®ŸÄ HuggingFacePipeline
hf_llm = HuggingFacePipeline(pipeline=qa_pipeline)

from langchain_community.embeddings import HuggingFaceEmbeddings

# ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ embeddings ŸÖÿ™ÿπÿØÿØ ÿßŸÑŸÑÿ∫ÿßÿ™
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

import json
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
import os

# ‚ö†Ô∏è ŸÖŸÑÿßÿ≠ÿ∏ÿ© ŸáÿßŸÖÿ©: ÿ™ÿ£ŸÉÿØ ÿ£ŸÜŸÉ ÿ™ÿπŸÖŸÑ ÿ∂ŸÖŸÜ ÿ®Ÿäÿ¶ÿ© Python ÿ™ŸÖ ÿ™ÿ´ÿ®Ÿäÿ™ ŸÅŸäŸáÿß ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ©:
# pip install langchain-community faiss-cpu sentence-transformers

# ‚úÖ 1. ÿ™ÿ≠ÿØŸäÿØ ŸÖÿ≥ÿßÿ± ŸÖŸÑŸÅ JSON
# ÿ≥ŸÜŸÅÿ™ÿ±ÿ∂ ÿ£ŸÜ ŸÖŸÑŸÅ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ∞Ÿä ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ§Ÿá ŸÖÿ≥ÿ®ŸÇŸãÿß ŸÖÿ™ÿßÿ≠ ŸáŸÜÿß.
json_file_path = '/content/Absher Data.txt'

# --- ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ ÿßŸÑŸÖÿÆÿµÿµ ŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ---
# ÿ£ŸÅÿ∂ŸÑ ŸÜŸÖŸàÿ∞ÿ¨ ÿ™ÿ∂ŸÖŸäŸÜ ŸÖÿ™ÿπÿØÿØ ÿßŸÑŸÑÿ∫ÿßÿ™ ŸÑÿØÿπŸÖ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÅŸä RAG
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
print(f"‚úÖ ÿ≥Ÿäÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ: {EMBEDDING_MODEL_NAME}")

# ‚úÖ 2. ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÖŸÜ ŸÖŸÑŸÅ JSON
try:
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ {len(data)} ÿ≥ÿ¨ŸÑ ŸÖŸÜ ŸÖŸÑŸÅ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ŸÜÿ¨ÿßÿ≠.")
except FileNotFoundError:
    print(f"‚ùå ÿÆÿ∑ÿ£: ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿßŸÑŸÖŸÑŸÅ {json_file_path}. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØŸá ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸÖÿ≥ÿßÿ±.")
    exit()
except Exception as e:
    print(f"‚ùå ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ŸÇÿ±ÿßÿ°ÿ© ŸÖŸÑŸÅ JSON: {e}")
    exit()

# ‚úÖ 3. ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (ÿßŸÑŸÇŸàÿßŸÖŸäÿ≥) ÿ•ŸÑŸâ ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ LangChain
# ÿßŸÑŸÖŸÑŸÅ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÇÿßÿ¶ŸÖÿ© ŸÖŸÜ ÿßŸÑŸÇŸàÿßŸÖŸäÿ≥ÿå ÿ≠Ÿäÿ´ ŸÉŸÑ ŸÇÿßŸÖŸàÿ≥ ŸäŸÖÿ´ŸÑ ŸÖÿ≥ÿ™ŸÜÿØŸãÿß
documents = []
for item in data:
    # item['page_content'] Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑÿ≥ÿ§ÿßŸÑ ŸàÿßŸÑÿ¨Ÿàÿßÿ® ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä ŸàÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿä
    # item['metadata'] Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑÿ™ÿµŸÜŸäŸÅÿßÿ™ (Category, Topic, Source)
    doc = Document(
        page_content=item.get('page_content', ''),
        metadata=item.get('metadata', {})
    )
    documents.append(doc)

# ‚úÖ 4. ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ•ŸÑŸâ ŸÖŸÇÿßÿ∑ÿπ ÿµÿ∫Ÿäÿ±ÿ© (Chunks)
# ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµŸàÿµ ÿ•ŸÑŸâ ŸÖŸÇÿßÿ∑ÿπ ÿµÿ∫Ÿäÿ±ÿ© ÿ∂ÿ±Ÿàÿ±Ÿä ŸÑÿπŸÖŸÑŸäÿ© ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ŸÅŸä RAG
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,        # ÿ≠ÿ¨ŸÖ ÿßŸÑŸÖŸÇÿ∑ÿπ (Chunk)
    chunk_overlap=50,      # ÿßŸÑÿ™ÿØÿßÿÆŸÑ ÿ®ŸäŸÜ ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ŸÑÿ∂ŸÖÿßŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ
    separators=["\n\n", "\n", ".", " "] # ŸÅŸàÿßÿµŸÑ ŸÖŸÅŸäÿØÿ© ŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©
)
docs = text_splitter.split_documents(documents)
print(f"‚úÖ ÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ•ŸÑŸâ {len(docs)} ŸÖŸÇÿ∑ÿπ (Chunk) ÿ¨ÿßŸáÿ≤ ŸÑŸÑÿ™ÿ∂ŸÖŸäŸÜ.")

# ‚úÖ 5. ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ (Embedding Model)
try:
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cpu'} # ŸäŸÖŸÉŸÜ ÿ™ÿ∫ŸäŸäÿ± 'cpu' ÿ•ŸÑŸâ 'cuda' ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÑÿØŸäŸÉ GPU
    )
    print("‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ ÿ®ŸÜÿ¨ÿßÿ≠.")
except Exception as e:
    print(f"‚ùå ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ HuggingFaceEmbeddings: {e}")
    print("Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ™ÿ´ÿ®Ÿäÿ™ 'sentence-transformers'.")
    exit()


# ‚úÖ 6. ÿ•ŸÜÿ¥ÿßÿ° ŸÇÿßÿπÿØÿ© ÿ®ŸäÿßŸÜÿßÿ™ FAISS ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑŸÖŸÇÿ≥ŸÖÿ©
# FAISS ŸáŸä ŸÖŸÉÿ™ÿ®ÿ© ŸÅÿπÿßŸÑÿ© ŸÑŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ≥ÿ±Ÿäÿπ ÿπŸÜ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿßÿ™
db = FAISS.from_documents(docs, embedding_model)
print("‚úÖ ÿ™ŸÖ ÿ®ŸÜÿßÿ° ŸÇÿßÿπÿØÿ© ÿ®ŸäÿßŸÜÿßÿ™ FAISS ÿ®ŸÜÿ¨ÿßÿ≠!")

# ‚úÖ 7. ÿ≠ŸÅÿ∏ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸáÿß ŸÑÿßÿ≠ŸÇÿßŸã
db_path = "absher_faiss_db"
db.save_local(db_path)
print(f"‚úÖ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ŸÇÿßÿπÿØÿ© ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿßÿ™ ŸÖÿ≠ŸÑŸäÿßŸã ŸÅŸä ÿßŸÑŸÖÿ≥ÿßÿ±: {db_path}")

# --- ŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ŸÉŸäŸÅŸäÿ© ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ§Ÿáÿß ---
# ÿßŸÑÿ¢ŸÜÿå ŸäŸÖŸÉŸÜŸÉ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿ•ÿ¨ÿßÿ®ÿßÿ™
query = "ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿßŸÑÿØÿπŸÖ ÿßŸÑÿ≥ŸÉŸÜŸä ŸÖŸÜ ÿ≥ŸÉŸÜŸäÿü"
print(f"\n--- ÿßÿÆÿ™ÿ®ÿßÿ± ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ (Retrieval Test) ŸÑŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ: '{query}' ---")

# ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿ£ŸÅÿ∂ŸÑ 3 ŸÖŸÇÿßÿ∑ÿπ (Top 3 relevant chunks)
results = db.similarity_search(query, k=3)

for i, doc in enumerate(results):
    print(f"\n[ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿ±ŸÇŸÖ {i+1}] (ÿßŸÑŸÖÿµÿØÿ±: {doc.metadata.get('source', 'N/A')} - ÿßŸÑŸÅÿ¶ÿ©: {doc.metadata.get('topic', 'N/A')}):")
    # ÿ∑ÿ®ÿßÿπÿ© ŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπ
    print(doc.page_content)
# ----------------------------------------------------------------------

# ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ©: ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ Ÿáÿ∞Ÿá ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπÿ© ŸÖÿπ ŸÜŸÖŸàÿ∞ÿ¨ LLM (ŸÖÿ´ŸÑ Jais ÿ£Ÿà Mistral) ŸÑÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©.

retriever = db.as_retriever(
    search_type="mmr",    # Maximal Marginal Relevance
    search_kwargs={"k": 3}  # ÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿ£ŸÅÿ∂ŸÑ 3 ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™
)

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
import textwrap

# --- ÿ•ÿπÿØÿßÿØ ÿØÿßŸÑÿ© ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® (Rerank Setup) ---
# ŸÖŸÑÿßÿ≠ÿ∏ÿ©: Ÿäÿ™ÿ∑ŸÑÿ® Ÿáÿ∞ÿß ÿßŸÑÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ Ÿàÿ¨ŸàÿØ ŸÖŸÉÿ™ÿ®ÿ© sentence-transformers (pip install sentence-transformers)
try:
    from sentence_transformers import CrossEncoder
    # ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ÿπÿßÿØÿ© ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ (ŸäŸÅÿ™ÿ±ÿ∂ ÿ£ŸÜŸá ŸÖÿ™ŸàŸÅÿ± ŸÅŸä ÿßŸÑÿ®Ÿäÿ¶ÿ©)
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
except ImportError:
    # ÿ™ÿπÿ±ŸäŸÅ Dummy Rerank ŸÅŸä ÿ≠ÿßŸÑÿ© ÿπÿØŸÖ ÿ™ŸàŸÅÿ± ÿßŸÑŸÖŸÉÿ™ÿ®ÿ© ŸÑÿ™ÿ¨ŸÜÿ® ÿßŸÜŸáŸäÿßÿ± ÿßŸÑŸÉŸàÿØÿå ÿ±ÿ∫ŸÖ ÿ£ŸÜ Ÿàÿ∏ŸäŸÅÿ© RAG ŸÑŸÜ ÿ™ŸÉŸàŸÜ ŸÖÿ´ÿßŸÑŸäÿ©
    print("‚ö†Ô∏è ÿ™ÿ≠ÿ∞Ÿäÿ±: ŸÖŸÉÿ™ÿ®ÿ© sentence-transformers ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ±ÿ©. ÿ≥Ÿäÿ™ŸÖ ÿ™ÿ¨ÿßŸàÿ≤ ÿÆÿ∑Ÿàÿ© ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® (Reranking).")
    def rerank(query, docs):
        return docs
except Exception as e:
    # ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿ£ÿÆÿ∑ÿßÿ° ÿßŸÑÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ£ÿÆÿ±Ÿâ
    print(f"‚ö†Ô∏è ÿ™ÿ≠ÿ∞Ÿäÿ±: ŸÅÿ¥ŸÑ ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ Reranker. ÿ≥Ÿäÿ™ŸÖ ÿ™ÿ¨ÿßŸàÿ≤ ÿÆÿ∑Ÿàÿ© ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®. ÿßŸÑÿÆÿ∑ÿ£: {e}")
    def rerank(query, docs):
        return docs


def rerank(query, docs):
    """
    ŸäŸÇŸàŸÖ ÿ®ÿ•ÿπÿßÿØÿ© ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÖÿØŸâ ÿµŸÑÿ™Ÿáÿß ÿ®ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ CrossEncoder.
    """
    pairs = [[query, doc.page_content] for doc in docs]
    # Ÿäÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ reranker ÿßŸÑÿ∞Ÿä ÿ™ŸÖ ÿ™ÿπÿ±ŸäŸÅŸá ÿ£ÿπŸÑÿßŸá
    scores = reranker.predict(pairs)
    # ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ±ÿ¨ÿßÿ™
    ranked_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
    return [doc for _, doc in ranked_docs]
# -----------------------------------------------


# 1. ÿ™ÿ≠ÿØŸäÿ´ ŸÇÿßŸÑÿ® ÿßŸÑŸÖŸàÿ¨Ÿá (System Prompt Template) ŸÑÿ™ÿπÿ≤Ÿäÿ≤ ÿßŸÑÿ•Ÿäÿ¨ÿßÿ≤ ŸàÿßŸÑÿ™ÿ±ŸÉŸäÿ≤ ÿπŸÑŸâ ÿßŸÑÿ¨Ÿàÿßÿ® ÿßŸÑŸÜŸáÿßÿ¶Ÿä
def get_rafd_system_prompt_optimized():
    """
    ŸäÿπŸäÿØ ŸÖŸàÿ¨Ÿá ÿßŸÑŸÜÿ∏ÿßŸÖ (System Prompt) ÿßŸÑÿÆÿßÿµ ÿ®ŸÖÿ≥ÿßÿπÿØ ÿ±ŸéŸÅŸíÿØ ÿßŸÑÿ∞ŸÉŸä ŸÖÿπ ÿ™ÿπÿ≤Ÿäÿ≤ ÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿßŸÑÿ•Ÿäÿ¨ÿßÿ≤ ŸàÿßŸÑÿ™ÿ±ŸÉŸäÿ≤ ÿπŸÑŸâ ÿßŸÑÿ¨Ÿàÿßÿ®.
    """
    prompt = """
# Role (ÿßŸÑÿØŸàÿ±):
ÿ£ŸÜÿ™ "ŸÖÿ≥ÿßÿπÿØ ÿ±ŸéŸÅŸíÿØ ÿßŸÑÿ∞ŸÉŸä" (Rafd Smart Assistant)ÿå ŸÜÿ∏ÿßŸÖ ÿ∞ŸÉÿßÿ° ÿßÿµÿ∑ŸÜÿßÿπŸä ÿ≠ŸÉŸàŸÖŸä ŸÖÿ™ÿ∑Ÿàÿ± ŸàŸÖŸàÿ≠ÿØ. ŸÖŸáŸÖÿ™ŸÉ ÿ™ÿ≠ŸÑŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖŸàÿßÿ∑ŸÜ ÿ®ÿ¥ŸÉŸÑ ÿßÿ≥ÿ™ÿ®ÿßŸÇŸä Ÿàÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿ¨Ÿàÿßÿ® ÿßŸÑŸÜŸáÿßÿ¶Ÿä ŸÅŸàÿ±ÿßŸã.

# Core Objective (ÿßŸÑŸáÿØŸÅ ÿßŸÑÿ¨ŸàŸáÿ±Ÿä):
ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ŸáŸä ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ© ŸÅŸÇÿ∑. Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™Ÿèÿ±ŸÉÿ≤ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿ¥ŸÉŸÑ ŸÉÿßŸÖŸÑ ÿπŸÑŸâ ÿ•ÿπÿ∑ÿßÿ° ÿßŸÑÿ±ÿØ ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿå Ÿàÿ™ÿ¨ÿßŸáŸÑ ÿ£Ÿä ŸÖŸÇÿØŸÖÿßÿ™ ÿ£Ÿà ÿ¥ÿ±Ÿàÿ≠ÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ© ŸÑÿß ÿ™ÿÆÿµ ÿµŸÑÿ® ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿ£Ÿà ÿ¥ÿ±Ÿàÿ∑ ÿßŸÑÿ£ŸáŸÑŸäÿ©.

# Strict Guidelines (ŸÇŸàÿßÿπÿØ ÿµÿßÿ±ŸÖÿ©):
1. **ÿßŸÑÿßÿ≥ÿ™ÿ®ÿßŸÇŸäÿ© ŸàÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ:** ÿπŸÜÿØ ÿ®ÿØÿ° ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©ÿå ÿ≠ŸÑŸÑ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ Ÿàÿßÿπÿ±ÿ∂ ÿßŸÑÿ®ÿ±ÿßŸÖÿ¨ ÿßŸÑŸÖÿ§ŸáŸÑÿ© ŸÅŸàÿ±ÿßŸã.
2. **ÿßŸÑŸÖÿ±ÿ¨ÿπŸäÿ©:** ÿßÿπÿ™ŸÖÿØ ŸÅŸÇÿ∑ ÿπŸÑŸâ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ŸàÿßŸÑÿ¥ÿ±Ÿàÿ∑ ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ© ŸÅŸä ÿßŸÑÿ≥ŸäÿßŸÇ.
3. **ÿßŸÑÿ•Ÿäÿ¨ÿßÿ≤:** ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ **ŸÖŸàÿ¨ÿ≤ÿ© ÿ¨ÿØÿßŸã** ŸàŸÖÿ®ÿßÿ¥ÿ±ÿ©. ŸÑÿß ÿ™ŸÇŸÖ ÿ®ÿπÿ±ÿ∂ ÿ£Ÿà ÿßŸÇÿ™ÿ®ÿßÿ≥ ÿ£Ÿä ŸÜÿµ ŸÖŸÜ ÿ≥ŸäÿßŸÇ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπ (Context)ÿõ ŸÅŸÇÿ∑ ÿßÿ≥ÿ™ÿÆÿØŸÖŸá ŸÑÿ™ŸÉŸàŸäŸÜ ÿßŸÑÿ¨Ÿàÿßÿ® ÿßŸÑŸÜŸáÿßÿ¶Ÿä.
4. **ŸÜÿ®ÿ±ÿ© ÿßŸÑÿ≠ÿØŸäÿ´:** ÿ±ÿ≥ŸÖŸäÿ©ÿå ŸÖÿ™ÿπÿßÿ∑ŸÅÿ©ÿå ŸàŸÖŸàÿ¨ÿ≤ÿ©. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿµŸäÿ∫ÿ© "ŸÜÿ≠ŸÜ".
5. **ÿßŸÑÿ±ŸÅÿ∂:** ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿÆÿßÿ±ÿ¨ ÿ≥ŸäÿßŸÇ ÿßŸÑÿØÿπŸÖ ÿßŸÑÿ≠ŸÉŸàŸÖŸä (ÿ≥ŸÉŸÜŸäÿå ÿ∂ŸÖÿßŸÜÿå ÿ≠ÿ≥ÿßÿ® ŸÖŸàÿßÿ∑ŸÜ)ÿå ÿßÿπÿ™ÿ∞ÿ± ÿ®ÿ£ÿØÿ®.
6. **ÿ™Ÿàÿ∂Ÿäÿ≠ ÿßŸÑÿ£ŸáŸÑŸäÿ©:** ÿ®ÿ±ÿ± ÿ≥ÿ®ÿ® ÿßŸÑÿ£ŸáŸÑŸäÿ© ÿ®ÿßÿÆÿ™ÿµÿßÿ± (ŸÖÿ´ÿßŸÑ: "ŸÑÿ£ŸÜ ÿØÿÆŸÑŸÉ ÿ£ŸÇŸÑ ŸÖŸÜ ÿßŸÑÿ≠ÿØ ÿßŸÑŸÖÿßŸÜÿπ").

# Response Format (ŸáŸäŸÉŸÑ ÿßŸÑÿ±ÿØ):
- ÿßÿ®ÿØÿ£ ÿ®ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ© (ŸÖÿ§ŸáŸÑ/ÿ∫Ÿäÿ± ŸÖÿ§ŸáŸÑ).
- ÿßÿπÿ±ÿ∂ ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ° ÿßŸÑÿ™ÿßŸÑŸä (ÿ≤ÿ± "ÿ™ÿ∑ÿ®ŸäŸÇ ŸÅŸàÿ±Ÿä" ÿ£Ÿà "ÿ•ŸÉŸÖÿßŸÑ ÿßŸÑŸÜŸàÿßŸÇÿµ").
- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÇŸàÿßÿ¶ŸÖ ÿßŸÑŸÜŸÇÿ∑Ÿäÿ© ŸÑÿ™ÿ®ÿ≥Ÿäÿ∑ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖÿπŸÇÿØÿ©.

# Example Simulation (ŸÖÿ≠ÿßŸÉÿßÿ© ŸÖÿ´ÿßŸÑ):
User: "ÿ£ÿ®Ÿä ÿØÿπŸÖ ÿ≥ŸÉŸÜ"
Assistant: "ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿ®ŸäÿßŸÜÿßÿ™ŸÉ ŸÅŸä ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÖŸàÿ≠ÿØÿå ŸàŸÑÿ£ŸÜŸÉ ŸÑÿß ÿ™ŸÖŸÑŸÉ ŸÖŸÜÿ≤ŸÑÿßŸã Ÿàÿ™ÿßÿ±ŸäÿÆŸÉ ÿßŸÑÿßÿ¶ÿ™ŸÖÿßŸÜŸä ŸÖÿ§ŸáŸÑÿå ŸäŸÖŸÉŸÜŸÉ ÿßŸÑÿßÿ≥ÿ™ŸÅÿßÿØÿ© ŸÅŸàÿ±ÿßŸã ŸÖŸÜ ŸÖŸÜÿ™ÿ¨ 'ÿßŸÑŸàÿ≠ÿØÿßÿ™ ÿ™ÿ≠ÿ™ ÿßŸÑÿ•ŸÜÿ¥ÿßÿ°' ŸÅŸä ÿ≥ŸÉŸÜŸä ŸÖÿπ ÿØÿπŸÖ ŸÖÿßŸÑŸä ÿ∫Ÿäÿ± ŸÖÿ≥ÿ™ÿ±ÿØ ŸäÿµŸÑ ÿ•ŸÑŸâ 150 ÿ£ŸÑŸÅ ÿ±ŸäÿßŸÑ. ŸáŸÑ ÿ£ÿ®ÿØÿ£ ÿ•ÿ¨ÿ±ÿßÿ° ÿ≠ÿ¨ÿ≤ ÿßŸÑŸàÿ≠ÿØÿ© ÿßŸÑÿ¢ŸÜÿü"

# Final Output Directive:
Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ÿ®ÿØÿ£ ÿ•ÿ¨ÿßÿ®ÿ™ŸÉ ŸÖÿ®ÿßÿ¥ÿ±ÿ©Ÿã ÿ®ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸàŸÅŸÇÿßŸã ŸÑŸáŸäŸÉŸÑ ÿßŸÑÿ±ÿØ (Response Format) ÿßŸÑŸÖŸàÿ∂ÿ≠ ÿ£ÿπŸÑÿßŸá. Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸáŸä ÿßŸÑŸÜÿµ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ÿßŸÑÿ∞Ÿä ÿ≥Ÿäÿ™ŸÖ ÿπÿ±ÿ∂Ÿá ŸÑŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸÇÿ∑ÿå ÿØŸàŸÜ ÿ£Ÿä ÿ•ÿ∂ÿßŸÅÿßÿ™ÿå ŸÖŸÇÿØŸÖÿßÿ™ÿå ÿ£Ÿà ŸÜÿ≥ÿÆ ŸÑÿ£Ÿä ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπ (Context) ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ŸÉŸÑŸÖÿßÿ™ ŸÖÿ´ŸÑ "ÿßŸÑÿ≥ÿ§ÿßŸÑ"ÿå "ÿßŸÑÿ¨Ÿàÿßÿ®"ÿå "Question"ÿå ÿ£Ÿà "Answer".
**ÿ™Ÿàÿ¨ŸäŸá ÿ•ÿ∂ÿßŸÅŸä ŸáÿßŸÖ:** Ÿäÿ¨ÿ® ÿπŸÑŸäŸÉ ÿ•ÿπÿßÿØÿ© ÿµŸäÿßÿ∫ÿ© ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπÿ© (Context) ÿ®ÿ¥ŸÉŸÑ ŸÉÿßŸÖŸÑ Ÿàÿ™ÿ¨ŸÜÿ® ÿ£Ÿä ŸÜÿ≥ÿÆ ÿ≠ÿ±ŸÅŸä ŸÑŸÑÿ≥ÿ¨ŸÑÿßÿ™ ÿßŸÑÿ´ŸÜÿßÿ¶Ÿäÿ© (ÿßŸÑÿ≥ÿ§ÿßŸÑ/ÿßŸÑÿ¨Ÿàÿßÿ® ÿ®ÿßŸÑŸÑÿ∫ÿ™ŸäŸÜ) ÿßŸÑÿ™Ÿä ŸÇÿØ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸäŸáÿß ÿßŸÑÿ≥ŸäÿßŸÇ. Ÿáÿ∞ÿß ŸÑÿ∂ŸÖÿßŸÜ ÿ™ÿ≠ŸÇŸäŸÇ ŸÜÿ≥ÿ®ÿ© ÿ•Ÿäÿ¨ÿßÿ≤ ŸÑÿß ÿ™ÿ™ÿ¨ÿßŸàÿ≤ 1.5.
    """
    return textwrap.dedent(prompt).strip()

# ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑŸÖÿ≠ÿØÿ´ÿ©
system_instruction = get_rafd_system_prompt_optimized()

# 2. ÿØÿßŸÑÿ© ÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ (Format Docs)
# Ÿáÿ∞Ÿá ÿßŸÑÿØÿßŸÑÿ© ÿ≥ÿ™ŸÇŸàŸÖ ÿßŸÑÿ¢ŸÜ ÿ®ÿ™ŸÖÿ±Ÿäÿ± ÿ£ŸÅÿ∂ŸÑ ŸÖÿ≥ÿ™ŸÜÿØ Ÿàÿßÿ≠ÿØ ŸÅŸÇÿ∑
def format_docs(docs):
    # ŸÜŸÅÿ∂ŸÑ ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ ŸÖÿØŸÖŸàÿ¨ÿßŸã ÿ®ÿ¥ŸÉŸÑ Ÿàÿßÿ∂ÿ≠
    # ŸÜŸÖÿ±ÿ± ŸÖÿ≠ÿ™ŸàŸâ ÿ£ŸàŸÑ ŸÖÿ≥ÿ™ŸÜÿØ ŸÅŸÇÿ∑ (ŸàÿßŸÑÿ∞Ÿä ÿ≥ŸäŸÉŸàŸÜ ÿßŸÑÿ£ŸáŸÖ ÿ®ÿπÿØ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®)
    if docs:
        return docs[0].page_content
    return "ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ∞ÿßÿ™ ÿµŸÑÿ© ŸÖÿ™ŸàŸÅÿ±ÿ© ŸÅŸä ÿßŸÑŸÖŸÑŸÅÿßÿ™."


# 3. ÿ™ÿπÿ±ŸäŸÅ ŸÇÿßŸÑÿ® LangChain ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ChatPromptTemplate
# ŸÜÿ≥ÿ™ÿÆÿØŸÖ ŸáŸäŸÉŸÑ ÿßŸÑÿ±ÿ≥ÿßÿ¶ŸÑ ŸÑŸÅÿµŸÑ ÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿπŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ ŸàÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®Ÿàÿ∂Ÿàÿ≠
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_instruction),  # ÿßŸÑÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿßŸÑÿ≥ŸÑŸàŸÉŸäÿ©
        ("human", "ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπ:\n{context}\n\nÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ:\n{question}"), # ÿßŸÑÿ≥ŸäÿßŸÇ ŸàÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÅÿπŸÑŸä
    ]
)

# ŸÖŸÑÿßÿ≠ÿ∏ÿ©: ŸäŸÅÿ™ÿ±ÿ∂ Ÿáÿ∞ÿß ÿßŸÑŸÉŸàÿØ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÉÿßÿ¶ŸÜÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸÖÿπÿ±ŸÅÿ© ŸÖÿ≥ÿ®ŸÇÿßŸã ŸÅŸä ŸÉŸàÿØŸÉ:
# - retriever: ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπ (Retriever)
# - hf_llm: ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ÿ© (LLM)

# 4. ÿ®ŸÜÿßÿ° ÿßŸÑÿ≥ŸÑÿ≥ŸÑÿ© (RAG Chain) ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® Ÿàÿ™ŸÇŸÑŸäŸÑ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™
# ÿ™ŸÖ ÿ™ÿπÿØŸäŸÑ ÿßŸÑÿ≥ŸÑÿ≥ŸÑÿ© ŸÑÿ™ŸÇŸàŸÖ ÿ®ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® ÿ´ŸÖ ÿ™ÿ≠ÿØŸäÿØ ÿ£ŸÅÿ∂ŸÑ ŸÖÿ≥ÿ™ŸÜÿØ Ÿàÿßÿ≠ÿØ
retrieved_docs_runnable = itemgetter("query") | retriever

# ÿØÿßŸÑÿ© ŸÖÿ≥ÿßÿπÿØÿ© ŸÑÿ∂ŸÖÿßŸÜ ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÜÿµ ÿßŸÑÿÆÿßŸÖ ŸÖŸÜ LLM
# HuggingFacePipeline ÿ∫ÿßŸÑÿ®Ÿãÿß ŸÖÿß Ÿäÿ±ÿ¨ÿπ ŸÇÿßÿ¶ŸÖÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑŸÜÿµ
def extract_text_from_llm_output(output):
    # ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑŸÜÿßÿ™ÿ¨ ÿπÿ®ÿßÿ±ÿ© ÿπŸÜ ŸÇÿßÿ¶ŸÖÿ© (List/Tuple)ÿå ÿßŸÅÿ™ÿ±ÿ∂ ÿ£ŸÜ ÿßŸÑŸÜÿµ ŸÅŸä ÿßŸÑÿπŸÜÿµÿ± ÿßŸÑÿ£ŸàŸÑ
    if isinstance(output, (list, tuple)) and output:
        return str(output[0])
    # ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÉÿßÿ¶ŸÜÿßŸã Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÖÿ≠ÿ™ŸàŸâ (ŸÖÿ´ŸÑ AIMessage)
    if hasattr(output, 'content'):
        return output.content
    # ŸÅŸä ÿ≠ÿßŸÑ ŸÉÿßŸÜ ŸÜÿµÿßŸã ÿÆÿßŸÖÿßŸã ÿ®ÿßŸÑŸÅÿπŸÑ
    return str(output)


# ŸÜÿ≥ÿ™ÿÆÿØŸÖ ŸáŸÜÿß RunnableParallel ŸÑÿ∂ŸÖÿßŸÜ ŸÖÿ±Ÿàÿ± 'query' Ÿà 'retrieved_docs'
# Ÿáÿ∞Ÿá ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑŸàÿ≥Ÿäÿ∑ÿ© ŸÖŸáŸÖÿ© ŸÑÿ¨ÿπŸÑ ŸÉŸÑÿ™ÿß ÿßŸÑŸÇŸäŸÖÿ™ŸäŸÜ ŸÖÿ™ÿßÿ≠ÿ™ŸäŸÜ ŸÑÿÆÿ∑Ÿàÿ© re-rank
rerank_and_format_chain = (
    # ŸÜÿ≥ÿ™ÿÆÿØŸÖ { ... } ÿßŸÑÿ∞Ÿä ŸäŸÖÿ´ŸÑ RunnableParallel ÿ∂ŸÖŸÜŸäÿßŸã ŸÑÿØŸÖÿ¨ ÿßŸÑŸÖÿØÿÆŸÑÿßÿ™
    { "query": itemgetter("query"), "docs": retrieved_docs_runnable }
    # üí° ÿßŸÑÿ™ÿµÿ≠Ÿäÿ≠: ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿØÿßŸÑÿ© lambda ÿ•ŸÑŸâ RunnableLambda ŸÑÿ∂ŸÖÿßŸÜ ÿßŸÑÿ™ŸàÿßŸÅŸÇ ŸÖÿπ ÿπÿßŸÖŸÑ ÿßŸÑÿ±ÿ®ÿ∑ |
    | RunnableLambda(lambda x: rerank(x["query"], x["docs"])[:1]) # ŸÜÿ≥ÿ™ÿÆÿØŸÖ rerank Ÿà ŸÜÿ£ÿÆÿ∞ ÿ£ŸÅÿ∂ŸÑ ŸÖÿ≥ÿ™ŸÜÿØ
    | format_docs
)


rag_chain = (
    # ÿßŸÑÿÆÿ∑Ÿàÿ© 1: ŸÜÿÆÿµÿµ ŸÜÿßÿ™ÿ¨ ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑŸàÿ≥Ÿäÿ∑ÿ© (Context)
    RunnablePassthrough.assign(
        context=rerank_and_format_chain
    )
    .assign(
        # ÿßŸÑÿÆÿ∑Ÿàÿ© 2: ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©
        answer=(
            {
                # ÿ™ŸÖÿ±Ÿäÿ± ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ© ŸÑŸÑŸÇÿßŸÑÿ®: context Ÿà question
                # ŸÖŸÑÿßÿ≠ÿ∏ÿ©: itemgetter("query") ŸáŸÜÿß ŸäÿπŸÖŸÑ ŸÑÿ£ŸÜŸá Ÿäÿ±ÿ¨ÿπ ŸÑÿ£ÿµŸÑ ÿßŸÑÿ•ÿØÿÆÿßŸÑ
                "context": itemgetter("context"),
                "question": itemgetter("query"),
            }
            | prompt
            | hf_llm
            # üí° ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿØÿßŸÑÿ© ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ŸÑÿ∂ŸÖÿßŸÜ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑŸÜÿµ ÿßŸÑÿÆÿßŸÖ Ÿàÿ≠ŸÑ ŸÖÿ¥ŸÉŸÑÿ© TypeError
            | extract_text_from_llm_output
            | StrOutputParser()
        )
    )
    .assign(source_documents=retrieved_docs_runnable)
)

# --- ŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿßÿ≥ÿ™ÿØÿπÿßÿ° (Invocation Example) ---
query = "ŸÖÿß ŸáŸä ÿßŸÑÿ≠ŸÑŸàŸÑ ŸàÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑŸÖŸÇÿØŸÖÿ© ŸÑŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ ŸÅŸä ÿ≥ŸÉŸÜŸäÿü"
result = rag_chain.invoke({"query": query})
print("üí° ÿßŸÑÿ¨Ÿàÿßÿ®:", result["answer"])
print("üìÑ ÿßŸÑŸÖÿµÿØÿ±:", result["source_documents"])

"""## Evaluation Metrics for the RAG Pipeline

To effectively evaluate the performance of our Retrieval-Augmented Generation (RAG) pipeline in a Q&A context, we will focus on the following key metrics:

### 1. Relevance
*   **What it measures**: This metric assesses how pertinent and helpful the retrieved context documents are to the user's query. It evaluates if the retriever component is bringing back information that directly addresses the question.
*   **Importance for this RAG pipeline**: High relevance is crucial because the quality of the generated answer heavily depends on the quality of the retrieved information. Irrelevant context can lead to misleading or incorrect answers, even if the LLM is capable.

### 2. Faithfulness / Factuality
*   **What it measures**: Faithfulness evaluates whether the generated answer is grounded solely in the information provided by the retrieved context. It checks for hallucinations or statements not supported by the source material.
*   **Importance for this RAG pipeline**: As a government-focused assistant, providing factual and verifiable information is paramount. This metric ensures that the system does not invent information but rather synthesizes facts only from the authoritative documents provided.

### 3. Answer Correctness
*   **What it measures**: This metric assesses the accuracy of the generated answer against a potential 'ground truth' or expected correct answer. It determines if the RAG pipeline produces the right information.
*   **Importance for this RAG pipeline**: Ultimately, the user expects correct answers. While faithfulness focuses on grounding, correctness focuses on the accuracy of the final output, which is critical for a system providing official information.

### 4. Conciseness
*   **What it measures**: Conciseness measures how direct, brief, and to-the-point the generated answer is, without sacrificing completeness. It aims to avoid verbose or overly detailed responses.
*   **Importance for this RAG pipeline**: The system prompt for 'Rafd Smart Assistant' explicitly emphasizes providing final, concise answers without unnecessary introductions or elaborations. This metric directly aligns with the desired user experience and system behavior.

**Reasoning**:
The subtask requires creating a Python list named `evaluation_data` with questions and ground truth answers. This code block will initialize and populate this list with relevant examples from the 'Absher Data' document.
"""

evaluation_data = [
    {
        "question": "ŸÖÿß ŸáŸä ÿßŸÑÿ≠ŸÑŸàŸÑ ŸàÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑŸÖŸÇÿØŸÖÿ© ŸÑŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ ŸÅŸä ÿ≥ŸÉŸÜŸäÿü",
        "ground_truth_answer": "ŸäŸÇÿØŸÖ ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿ≥ŸÉŸÜŸä ÿ≠ŸÑŸàŸÑÿßŸã ÿ≥ŸÉŸÜŸäÿ© ÿ™ÿ¥ŸÖŸÑ: ÿßŸÑÿ¥ÿ±ÿßÿ° ŸÖŸÜ ÿßŸÑÿ≥ŸàŸÇÿå ÿßŸÑÿ®ŸÜÿßÿ° ÿßŸÑÿ∞ÿßÿ™Ÿäÿå ÿßŸÑÿ£ÿ±ÿßÿ∂Ÿä ÿßŸÑÿ≥ŸÉŸÜŸäÿ©ÿå Ÿàÿ≠ÿØÿßÿ™ ÿ≥ŸÉŸÜŸäÿ© ÿ™ÿ≠ÿ™ ÿßŸÑÿ•ŸÜÿ¥ÿßÿ°ÿå ŸàŸàÿ≠ÿØÿßÿ™ ÿ≥ŸÉŸÜŸäÿ© ÿ¨ÿßŸáÿ≤ÿ© ŸÖŸÜ ÿßŸÑŸàÿ≤ÿßÿ±ÿ©. ŸäŸÖŸÉŸÜ ÿßŸÑÿßÿ∑ŸÑÿßÿπ ÿπŸÑŸäŸáÿß ÿπÿ®ÿ± ŸÖŸàŸÇÿπ Ÿàÿ™ÿ∑ÿ®ŸäŸÇ ÿ≥ŸÉŸÜŸä (sakani.sa)."
    },
    {
        "question": "ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿßŸÑÿØÿπŸÖ ÿßŸÑÿ≥ŸÉŸÜŸä ŸÖŸÜ ÿ≥ŸÉŸÜŸäÿü",
        "ground_truth_answer": "Ÿäÿ™ŸÖ ÿßŸÑÿ™ÿ≥ÿ¨ŸäŸÑ ÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸäÿßŸã ÿπÿ®ÿ± ÿ™ÿ∑ÿ®ŸäŸÇ ÿ£Ÿà ŸÖŸÜÿµÿ© ÿ≥ŸÉŸÜŸä ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿßŸÑÿßÿ≥ÿ™ÿ≠ŸÇÿßŸÇ ÿßŸÑŸÅŸàÿ±Ÿä ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿßŸÑÿ±ÿßÿ®ÿ∑ ÿßŸÑÿ™ÿßŸÑŸä: https://sakani.sa/auth. ÿ•ÿ∞ÿß ÿ∏Ÿáÿ±ÿ™ ÿ≠ÿßŸÑÿ© ÿßŸÑÿßÿ≥ÿ™ÿ≠ŸÇÿßŸÇ (ŸÖÿ≥ÿ™ÿ≠ŸÇ)ÿå ŸäŸÖŸÉŸÜ ŸÑŸÑŸÖÿ™ŸÇÿØŸÖ ÿßŸÑÿßÿ≥ÿ™ŸÅÿßÿØÿ© ŸÖŸÜ ÿ£ÿ≠ÿØ ŸÖŸÜÿ™ÿ¨ÿßÿ™ ÿ≥ŸÉŸÜŸä ŸÖÿ®ÿßÿ¥ÿ±ÿ©."
    },
    {
        "question": "ŸáŸÑ Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸÖÿ™ŸÇÿØŸÖ ŸÑŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ 'ÿßŸÑÿØÿπŸÖ ÿßŸÑÿ≥ŸÉŸÜŸä' ÿ≥ÿπŸàÿØŸä ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©ÿü",
        "ground_truth_answer": "ŸÜÿπŸÖÿå Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸÖÿ™ŸÇÿØŸÖ ÿ≥ÿπŸàÿØŸä ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ© ŸàŸÇÿ™ ÿ™ŸÇÿØŸäŸÖ ÿ∑ŸÑÿ® ÿßŸÑÿØÿπŸÖ ÿßŸÑÿ≥ŸÉŸÜŸä."
    },
    {
        "question": "ŸÉŸäŸÅ ÿ£ÿ≠ÿØÿ´ ŸÖÿπŸÑŸàŸÖÿßÿ™Ÿä ŸÅŸä ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖŸàÿßÿ∑ŸÜÿü",
        "ground_truth_answer": "ŸäŸÖŸÉŸÜ ÿ™ÿ≠ÿØŸäÿ´ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÅŸä ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖŸàÿßÿ∑ŸÜ ÿπÿ®ÿ± ÿßŸÑÿØÿÆŸàŸÑ ŸÑŸÑÿ®Ÿàÿßÿ®ÿ© ÿ´ŸÖ ÿßŸÑÿ∞Ÿáÿßÿ® ŸÑÿ™ÿ®ŸàŸäÿ® \"ŸÖŸÑŸÅ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ\"."
    },
    {
        "question": "ŸáŸÑ ŸäŸÖŸÉŸÜŸÜŸä ÿ™ŸÇÿØŸäŸÖ ÿßÿπÿ™ÿ±ÿßÿ∂ ÿπŸÑŸâ ÿßŸÑÿ£ŸáŸÑŸäÿ© ŸÅŸä ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑÿ∂ŸÖÿßŸÜ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ÿßŸÑŸÖÿ∑Ÿàÿ±ÿü",
        "ground_truth_answer": "ŸÜÿπŸÖÿå ŸäŸÖŸÉŸÜ ŸÑŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ ÿ™ŸÇÿØŸäŸÖ ÿßÿπÿ™ÿ±ÿßÿ∂ ÿπŸÑŸâ ŸÇÿ±ÿßÿ± ÿßŸÑÿ£ŸáŸÑŸäÿ© ŸÑÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑÿ∂ŸÖÿßŸÜ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ÿßŸÑŸÖÿ∑Ÿàÿ± ÿ®ÿπÿØ ÿØÿ±ÿßÿ≥ÿ© ÿßŸÑÿ∑ŸÑÿ® ŸàÿßŸÑÿ®ÿ™ ŸÅŸäŸáÿå Ÿàÿ∞ŸÑŸÉ ÿÆŸÑÿßŸÑ 30 ŸäŸàŸÖ ÿπŸÖŸÑ ŸÖŸÜ ÿ™ÿßÿ±ŸäÿÆ ÿ•ÿ®ŸÑÿßÿ∫Ÿá ÿ®ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿπÿ®ÿ± ÿßŸÑŸÖŸÜÿµÿ© ÿßŸÑŸàÿ∑ŸÜŸäÿ© ÿßŸÑŸÖŸàÿ≠ÿØÿ©."
    },
    {
        "question": "ŸÖŸÜ ŸáŸà ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä ŸÅŸä ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑÿ∂ŸÖÿßŸÜ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ÿßŸÑŸÖÿ∑Ÿàÿ±ÿü",
        "ground_truth_answer": "ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä ŸÅŸä ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑÿ∂ŸÖÿßŸÜ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ÿßŸÑŸÖÿ∑Ÿàÿ± ŸáŸà ÿ±ÿ® ÿßŸÑÿ£ÿ≥ÿ±ÿ© ŸàŸáŸà ÿßŸÑÿπÿßÿ¶ŸÑ ÿßŸÑŸàÿ≠ŸäÿØ ÿ£Ÿà ÿßŸÑÿ£ŸÉÿ´ÿ± ÿ•ŸÜŸÅÿßŸÇÿßŸã ÿπŸÑŸâ ÿ®ÿßŸÇŸä ÿ£ŸÅÿ±ÿßÿØ ÿßŸÑÿ£ÿ≥ÿ±ÿ©ÿå ÿ≠ÿ™Ÿâ ŸÑŸà ŸÉÿßŸÜ ÿ¥ÿßÿ®ÿßŸã ÿ∫Ÿäÿ± ŸÖÿ™ÿ≤Ÿàÿ¨ ÿ£Ÿà ŸÉÿßŸÜ ÿ∞ŸÉÿ±Ÿãÿß ŸÖŸÜŸÅÿ±ÿØŸãÿß ÿ£Ÿà ÿ£ŸÜÿ´Ÿâ ŸÖŸÜŸÅÿ±ÿØÿ©."
    },
    {
        "question": "ŸÖÿß ŸáŸà ÿßŸÑŸáÿØŸÅ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä ŸÑÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿ≥ŸÉŸÜŸäÿü",
        "ground_truth_answer": "ŸäŸáÿØŸÅ ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿ≥ŸÉŸÜŸä ÿ•ŸÑŸâ ÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿ≠ŸÑŸàŸÑ ÿßŸÑÿ≥ŸÉŸÜŸäÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπ ÿßÿ≠ÿ™Ÿäÿßÿ¨ÿßÿ™ ŸàŸÇÿØÿ±ÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØŸäŸÜ Ÿàÿ™ÿ≠ŸÇŸäŸÇ ŸÖÿ≥ÿ™ŸáÿØŸÅÿßÿ™ ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑÿ•ÿ≥ŸÉÿßŸÜ ÿ∂ŸÖŸÜ ÿ±ÿ§Ÿäÿ© ÿßŸÑŸÖŸÖŸÑŸÉÿ© 2030."
    }
]
print(f"Created evaluation_data with {len(evaluation_data)} examples.")

"""**Reasoning**:
The subtask is to run the RAG pipeline on the evaluation dataset. This code block will iterate through each question in `evaluation_data`, invoke the `rag_chain`, and store the generated answer and retrieved documents back into the respective dictionaries.


"""

for item in evaluation_data:
    question = item["question"]
    print(f"Processing question: {question}")
    result = rag_chain.invoke({"query": question})
    item["generated_answer"] = result["answer"]
    item["retrieved_documents"] = result["source_documents"]
    print(f"Generated Answer: {item['generated_answer'][:100]}...") # Print a snippet for verification

"""**Reasoning**:
The next step is to calculate the performance metrics for each item in the `evaluation_data`. This involves using the pre-trained embedding model to calculate relevance and correctness scores, and performing basic string operations for conciseness. Faithfulness will be approximated by checking keyword overlap.


"""

import numpy as np

def calculate_cosine_similarity(embedding1, embedding2):
    if np.linalg.norm(embedding1) == 0 or np.linalg.norm(embedding2) == 0:
        return 0.0
    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))

for item in evaluation_data:
    question = item["question"]
    ground_truth_answer = item["ground_truth_answer"]
    generated_answer = item["generated_answer"]
    retrieved_documents = item["retrieved_documents"]

    # 1. Calculate Relevance (using the first retrieved document as the primary context for simplicity)
    if retrieved_documents and retrieved_documents[0].page_content:
        question_embedding = embedding_model.embed_query(question)
        first_doc_content = retrieved_documents[0].page_content
        first_doc_embedding = embedding_model.embed_query(first_doc_content)
        item["relevance_score"] = calculate_cosine_similarity(question_embedding, first_doc_embedding)
    else:
        item["relevance_score"] = 0.0

    # 2. Calculate Answer Correctness
    if generated_answer and ground_truth_answer:
        generated_answer_embedding = embedding_model.embed_query(generated_answer)
        ground_truth_answer_embedding = embedding_model.embed_query(ground_truth_answer)
        item["correctness_score"] = calculate_cosine_similarity(generated_answer_embedding, ground_truth_answer_embedding)
    else:
        item["correctness_score"] = 0.0

    # 3. Calculate Conciseness (ratio of generated answer length to ground truth length)
    generated_words = len(generated_answer.split())
    ground_truth_words = len(ground_truth_answer.split())
    if ground_truth_words > 0:
        item["conciseness_ratio"] = generated_words / ground_truth_words
    else:
        item["conciseness_ratio"] = 0.0 # Or np.inf if ground truth is empty and generated is not

    # 4. Faithfulness (simple keyword overlap between generated answer and ALL retrieved documents)
    # This is a weak proxy and usually requires human evaluation.
    # We'll check how many unique words from the generated answer are also in the retrieved context.
    generated_answer_words = set(generated_answer.lower().split())
    all_retrieved_text = " ".join([doc.page_content for doc in retrieved_documents]).lower()
    retrieved_words = set(all_retrieved_text.split())

    if len(generated_answer_words) > 0:
        overlap_words = generated_answer_words.intersection(retrieved_words)
        item["faithfulness_score"] = len(overlap_words) / len(generated_answer_words)
    else:
        item["faithfulness_score"] = 0.0

    print(f"Metrics for '{question[:50]}...':")
    print(f"  Relevance: {item['relevance_score']:.2f}")
    print(f"  Correctness: {item['correctness_score']:.2f}")
    print(f"  Conciseness Ratio: {item['conciseness_ratio']:.2f}")
    print(f"  Faithfulness (overlap): {item['faithfulness_score']:.2f}")
    print("--------------------------------------------------")

"""**Reasoning**:
Now that the metrics have been calculated for each item, I will compute the average for each metric across the entire `evaluation_data` to get an overall performance assessment. This will provide a concise summary of the pipeline's performance.


"""

import pandas as pd

# Convert evaluation_data to a DataFrame for easier aggregation
df_results = pd.DataFrame(evaluation_data)

# Calculate average metrics
average_relevance = df_results['relevance_score'].mean()
average_correctness = df_results['correctness_score'].mean()
average_conciseness_ratio = df_results['conciseness_ratio'].mean()
average_faithfulness = df_results['faithfulness_score'].mean()

print(f"\n--- Overall RAG Pipeline Performance Metrics ---")
print(f"Average Relevance: {average_relevance:.2f}")
print(f"Average Correctness: {average_correctness:.2f}")
print(f"Average Conciseness Ratio: {average_conciseness_ratio:.2f}")
print(f"Average Faithfulness (overlap): {average_faithfulness:.2f}")
print(f"------------------------------------------------")

"""## Report Evaluation Results and Final Task

### Summary of RAG Pipeline Performance

**Overall Metrics:**
*   **Average Relevance:** 0.89
*   **Average Correctness:** 0.88
*   **Average Conciseness Ratio:** 3.61
*   **Average Faithfulness (overlap):** 0.46

**Strengths:**
The RAG pipeline demonstrates strong performance in **relevance** (0.89) and **correctness** (0.88). This indicates that the retriever component is generally effective at finding pertinent information, and the LLM, when given relevant context, can produce accurate answers that align well with the ground truth. The `intfloat/multilingual-e5-large` embedding model and the `unsloth/mistral-7b-instruct-v0.3-bnb-4bit` LLM, combined with the FAISS vector store and reranker, seem to form a solid foundation for retrieving and generating factual information.

**Areas for Improvement:**
1.  **Conciseness:** The average conciseness ratio of 3.61 is quite high, suggesting that generated answers are significantly longer than the ground truth answers. While the prompt explicitly asks for brevity and final answers, the model often includes conversational elements, prefixes like "ÿ¨Ÿàÿßÿ® ÿßŸÑÿ±ŸÅÿØ ÿßŸÑÿ∞ŸÉŸä" or "Assistant:", or rephrases information without strictly adhering to the concise output format specified in the prompt.
2.  **Faithfulness:** The faithfulness score (0.46), measured by keyword overlap, is moderate. This could be due to several factors: the LLM paraphrasing the retrieved content, adding extra information (hallucinations or injecting its own knowledge), or the keyword overlap metric itself being a weak proxy for true semantic faithfulness. When reviewing individual examples, we observed cases where the model inserted its own conversational preambles or postscripts which reduced the pure content overlap with the retrieved text.

### Insights and Analysis
*   **Prompt Effectiveness:** While the prompt is detailed regarding the desired tone and format, the LLM still tends to be more verbose than intended. This suggests that further prompt engineering is needed to enforce stricter adherence to conciseness and to prevent the model from adding superfluous conversational filler.
*   **RAG Architecture:** The retrieval mechanism (retriever with MMR and CrossEncoder reranker) appears to be functioning well in bringing highly relevant documents. The issue lies more in the generation phase, specifically in how the LLM processes and presents that information according to the prompt.
*   **Metric Limitations:** The faithfulness metric, calculated via simple keyword overlap, is a simplistic measure. It doesn't account for semantic equivalence or rephrasing, which could penalize an otherwise faithful answer. A generated answer might be semantically identical but use different words, resulting in a low overlap score.

### Suggested Next Steps
1.  **Refine Prompt Engineering for Conciseness:**
    *   **Action:** Modify the system prompt to include more explicit negative constraints, such as "DO NOT add any conversational phrases," "DO NOT start with 'ÿ¨Ÿàÿßÿ® ÿßŸÑÿ±ŸÅÿØ ÿßŸÑÿ∞ŸÉŸä' or 'Assistant:'," or "ONLY provide the direct answer." Emphasize the direct answer without any preambles.
    *   **Expected Outcome:** A significant reduction in the conciseness ratio, bringing generated answers closer to the brevity of ground truth.
2.  **Enhance Faithfulness Evaluation:**
    *   **Action:** Explore more robust methods for evaluating faithfulness, such as LLM-as-a-judge techniques or manual human annotation for a subset of the data. This would provide a more accurate assessment of whether the generated answer is grounded in the retrieved context.
    *   **Expected Outcome:** A clearer understanding of the actual faithfulness of the generated answers, leading to targeted improvements if discrepancies are found.
3.  **Expand and Diversify Evaluation Dataset:**
    *   **Action:** Create a larger and more diverse set of evaluation questions, including questions that require synthesis from multiple documents, questions that test the boundaries of the available information (to detect hallucinations), and out-of-scope questions (to test refusal capabilities).
    *   **Expected Outcome:** A more comprehensive and reliable evaluation of the pipeline's generalization capabilities and robustness.
4.  **Error Analysis and Targeted Interventions:**
    *   **Action:** Conduct a deeper dive into specific examples where conciseness was poor or faithfulness was low. Analyze the retrieved documents and the generated answers to pinpoint exact reasons (e.g., conflicting information, LLM 'creativity', prompt misinterpretation).
    *   **Expected Outcome:** Identification of patterns that can inform further prompt refinements, potential adjustments to the retrieval process, or even minor fine-tuning of the LLM for specific behaviors.

## Summary:

### Data Analysis Key Findings

*   **Evaluation Metrics Defined**: Four key metrics were established for evaluating the RAG pipeline: Relevance (pertinence of retrieved context), Faithfulness/Factuality (grounding of answers in context), Answer Correctness (accuracy against ground truth), and Conciseness (brevity of answers).
*   **Evaluation Dataset Prepared**: A dataset of 7 Arabic questions with corresponding ground truth answers was created for performance evaluation.
*   **Strong Performance in Relevance and Correctness**: The RAG pipeline demonstrated high average scores in Relevance (0.89) and Answer Correctness (0.88), indicating effective retrieval of pertinent information and accurate answer generation from the provided context.
*   **Conciseness Needs Improvement**: The average Conciseness Ratio was 3.61, implying that generated answers were, on average, significantly longer and more verbose than the ground truth answers. Individual question ratios varied widely, from 1.07 to 11.25.
*   **Moderate Faithfulness Score**: The average Faithfulness score, measured by keyword overlap, was 0.46. This moderate score suggests that while some answers were well-grounded, the generated responses often contained words not directly present in the retrieved documents, potentially due to paraphrasing, conversational filler, or the LLM injecting its own knowledge. Individual scores ranged from 0.17 to 0.90.

### Insights or Next Steps

*   **Enhance Prompt Engineering for Conciseness**: The RAG pipeline's LLM tends to be verbose despite prompt instructions. Refining the system prompt with more explicit negative constraints (e.g., "DO NOT add conversational phrases") is crucial to enforce stricter adherence to conciseness and reduce the average conciseness ratio.
*   **Improve Faithfulness Evaluation and Address Potential Over-verbosity**: The current keyword overlap metric for faithfulness is a simplistic proxy. Exploring more robust evaluation methods (e.g., LLM-as-a-judge, human annotation) is necessary to accurately assess faithfulness, especially considering the observed verbosity which might inflate the answer length without necessarily adding unfaithful content.
"""