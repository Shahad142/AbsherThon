# -*- coding: utf-8 -*-
"""Rafad LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TP2HFi06q5h4DKxKHtwRowU7vNUxC0yl
"""

from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

# ุฅุนุฏุงุฏ ุงูู pipeline ูุน ุงููููุฐุฌ
qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=False,  # ุงุณุชุฎุฏุงู Greedy decoding
    return_full_text=False # ุฅุถุงูุฉ ูุฐุง ูุถูุงู ุฅุฑุฌุงุน ุงููุต ุงูุฌุฏูุฏ ููุท
)

# ููู ุจู HuggingFacePipeline
hf_llm = HuggingFacePipeline(pipeline=qa_pipeline)

from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

# ุฅุนุฏุงุฏ ุงูู pipeline ูุน ุงููููุฐุฌ
qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=False,  # ุงุณุชุฎุฏุงู Greedy decoding
    return_full_text=False # ุฅุถุงูุฉ ูุฐุง ูุถูุงู ุฅุฑุฌุงุน ุงููุต ุงูุฌุฏูุฏ ููุท
)

# ููู ุจู HuggingFacePipeline
hf_llm = HuggingFacePipeline(pipeline=qa_pipeline)

!pip install langchain_community

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
# !pip install langchain faiss-cpu sentence-transformers
# !pip install transformers accelerate bitsandbytes
#

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",  # Instruct fine-tuned model
    max_seq_length = 4096, # Increased max_seq_length to accommodate longer inputs
    dtype = None,
    load_in_4bit = True,
)

# ุชุฌููุฒ ุงููููุฐุฌ ููุชูููุฏ
FastLanguageModel.for_inference(model)

from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

# ุฅุนุฏุงุฏ ุงูู pipeline ูุน ุงููููุฐุฌ
qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=False,  # ุงุณุชุฎุฏุงู Greedy decoding
    return_full_text=False # ุฅุถุงูุฉ ูุฐุง ูุถูุงู ุฅุฑุฌุงุน ุงููุต ุงูุฌุฏูุฏ ููุท
)

# ููู ุจู HuggingFacePipeline
hf_llm = HuggingFacePipeline(pipeline=qa_pipeline)

from langchain_community.embeddings import HuggingFaceEmbeddings

# ุชุญููู ูููุฐุฌ embeddings ูุชุนุฏุฏ ุงููุบุงุช
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

import json
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
import os

# โ๏ธ ููุงุญุธุฉ ูุงูุฉ: ุชุฃูุฏ ุฃูู ุชุนูู ุถูู ุจูุฆุฉ Python ุชู ุชุซุจูุช ูููุง ุงูููุชุจุงุช ุงูุชุงููุฉ:
# pip install langchain-community faiss-cpu sentence-transformers

# โ 1. ุชุญุฏูุฏ ูุณุงุฑ ููู JSON
# ุณููุชุฑุถ ุฃู ููู ุงูุจูุงูุงุช ุงูุฐู ุชู ุฅูุดุงุคู ูุณุจููุง ูุชุงุญ ููุง.
json_file_path = '/content/Absher Data.txt'

# --- ูููุฐุฌ ุงูุชุถููู ุงููุฎุตุต ููุบุฉ ุงูุนุฑุจูุฉ ---
# ุฃูุถู ูููุฐุฌ ุชุถููู ูุชุนุฏุฏ ุงููุบุงุช ูุฏุนู ุงููุบุฉ ุงูุนุฑุจูุฉ ูู RAG
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
print(f"โ ุณูุชู ุงุณุชุฎุฏุงู ูููุฐุฌ ุงูุชุถููู: {EMBEDDING_MODEL_NAME}")

# โ 2. ุชุญููู ุงูุจูุงูุงุช ูู ููู JSON
try:
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"โ ุชู ุชุญููู {len(data)} ุณุฌู ูู ููู ุงูุจูุงูุงุช ุจูุฌุงุญ.")
except FileNotFoundError:
    print(f"โ ุฎุทุฃ: ูู ูุชู ุงูุนุซูุฑ ุนูู ุงูููู {json_file_path}. ูุฑุฌู ุงูุชุฃูุฏ ูู ูุฌูุฏู ูู ููุณ ุงููุณุงุฑ.")
    exit()
except Exception as e:
    print(f"โ ุฎุทุฃ ุฃุซูุงุก ูุฑุงุกุฉ ููู JSON: {e}")
    exit()

# โ 3. ุชุญููู ุงูุจูุงูุงุช (ุงูููุงููุณ) ุฅูู ูุณุชูุฏุงุช LangChain
# ุงูููู ูุญุชูู ุนูู ูุงุฆูุฉ ูู ุงูููุงููุณุ ุญูุซ ูู ูุงููุณ ููุซู ูุณุชูุฏูุง
documents = []
for item in data:
    # item['page_content'] ูุญุชูู ุนูู ุงูุณุคุงู ูุงูุฌูุงุจ ุจุงูุนุฑุจู ูุงูุฅูุฌููุฒู
    # item['metadata'] ูุญุชูู ุนูู ุงูุชุตูููุงุช (Category, Topic, Source)
    doc = Document(
        page_content=item.get('page_content', ''),
        metadata=item.get('metadata', {})
    )
    documents.append(doc)

# โ 4. ุชูุณูู ุงููุณุชูุฏุงุช ุฅูู ููุงุทุน ุตุบูุฑุฉ (Chunks)
# ุชูุณูู ุงููุตูุต ุฅูู ููุงุทุน ุตุบูุฑุฉ ุถุฑูุฑู ูุนูููุฉ ุงูุงุณุชุฑุฌุงุน ูู RAG
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,        # ุญุฌู ุงูููุทุน (Chunk)
    chunk_overlap=50,      # ุงูุชุฏุงุฎู ุจูู ุงูููุงุทุน ูุถูุงู ุงูุณูุงู
    separators=["\n\n", "\n", ".", " "] # ููุงุตู ูููุฏุฉ ููุบุฉ ุงูุนุฑุจูุฉ
)
docs = text_splitter.split_documents(documents)
print(f"โ ุชู ุชูุณูู ุงููุณุชูุฏุงุช ุฅูู {len(docs)} ููุทุน (Chunk) ุฌุงูุฒ ููุชุถููู.")

# โ 5. ุชุญููู ูููุฐุฌ ุงูุชุถููู (Embedding Model)
try:
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cpu'} # ูููู ุชุบููุฑ 'cpu' ุฅูู 'cuda' ุฅุฐุง ูุงู ูุฏูู GPU
    )
    print("โ ุชู ุชุญููู ูููุฐุฌ ุงูุชุถููู ุจูุฌุงุญ.")
except Exception as e:
    print(f"โ ุฎุทุฃ ูู ุชุญููู ูููุฐุฌ HuggingFaceEmbeddings: {e}")
    print("ูุฑุฌู ุงูุชุฃูุฏ ูู ุชุซุจูุช 'sentence-transformers'.")
    exit()


# โ 6. ุฅูุดุงุก ูุงุนุฏุฉ ุจูุงูุงุช FAISS ูู ุงููุณุชูุฏุงุช ุงูููุณูุฉ
# FAISS ูู ููุชุจุฉ ูุนุงูุฉ ููุจุญุซ ุงูุณุฑูุน ุนู ุงููุชุฌูุงุช
db = FAISS.from_documents(docs, embedding_model)
print("โ ุชู ุจูุงุก ูุงุนุฏุฉ ุจูุงูุงุช FAISS ุจูุฌุงุญ!")

# โ 7. ุญูุธ ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุงุณุชุฎุฏุงููุง ูุงุญูุงู
db_path = "absher_faiss_db"
db.save_local(db_path)
print(f"โ ุชู ุญูุธ ูุงุนุฏุฉ ุจูุงูุงุช ุงููุชุฌูุงุช ูุญููุงู ูู ุงููุณุงุฑ: {db_path}")

# --- ูุซุงู ุนูู ููููุฉ ุงุณุชุฎุฏุงู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุชู ุชู ุฅูุดุงุคูุง ---
# ุงูุขูุ ููููู ุงุณุชุฎุฏุงู ูุงุนุฏุฉ ุงูุจูุงูุงุช ููุจุญุซ ุนู ุฅุฌุงุจุงุช
query = "ููู ูููููู ุงูุญุตูู ุนูู ุงูุฏุนู ุงูุณููู ูู ุณูููุ"
print(f"\n--- ุงุฎุชุจุงุฑ ุงูุงุณุชุฑุฌุงุน (Retrieval Test) ููุงุณุชุนูุงู: '{query}' ---")

# ุงุณุชุฑุฌุงุน ุฃูุถู 3 ููุงุทุน (Top 3 relevant chunks)
results = db.similarity_search(query, k=3)

for i, doc in enumerate(results):
    print(f"\n[ุงููุชูุฌุฉ ุฑูู {i+1}] (ุงููุตุฏุฑ: {doc.metadata.get('source', 'N/A')} - ุงููุฆุฉ: {doc.metadata.get('topic', 'N/A')}):")
    # ุทุจุงุนุฉ ูุญุชูู ุงููุต ุงููุณุชุฑุฌุน
    print(doc.page_content)
# ----------------------------------------------------------------------

# ุงูุฎุทูุฉ ุงูุชุงููุฉ: ุงุณุชุฎุฏุงู ูุฐู ุงูููุงุทุน ุงููุณุชุฑุฌุนุฉ ูุน ูููุฐุฌ LLM (ูุซู Jais ุฃู Mistral) ูุชูููุฏ ุงูุฅุฌุงุจุฉ ุงูููุงุฆูุฉ.

retriever = db.as_retriever(
    search_type="mmr",    # Maximal Marginal Relevance
    search_kwargs={"k": 3}  # ุงุณุชุฑุฌุงุน ุฃูุถู 3 ูุณุชูุฏุงุช
)

!pip install --upgrade langchain langchain-community langchain-core

import textwrap

def get_rafd_system_prompt():
    """
    ูุนูุฏ ููุฌู ุงููุธุงู (System Prompt) ุงูุฎุงุต ุจูุณุงุนุฏ ุฑูููุฏ ุงูุฐูู.
    ูุตูู ููุงุณุชุฎุฏุงู ูุน ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ (LLMs) ูู ุฃูุธูุฉ RAG.
    """

    # ุงุณุชุฎุฏุงู Triple Quotes ููุญูุงุธ ุนูู ุชูุณูู Markdown ูุงูุฃุณุทุฑ ุงูุฌุฏูุฏุฉ
    prompt = """
# Role (ุงูุฏูุฑ):
ุฃูุช "ูุณุงุนุฏ ุฑูููุฏ ุงูุฐูู" (Rafd Smart Assistant)ุ ูุธุงู ุฐูุงุก ุงุตุทูุงุนู ุญูููู ูุชุทูุฑ ูููุญุฏ. ูููุชู ููุณุช ูุฌุฑุฏ ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉุ ุจู ุชุญููู ุจูุงูุงุช ุงูููุงุทู ุงูุญุงููุฉ ุจุดูู ุงุณุชุจุงููุ ููุทุงุจูุชูุง ูุน ุดุฑูุท ุจุฑุงูุฌ ุงูุฏุนู (ุณูููุ ุงูุถูุงู ุงูุงุฌุชูุงุนูุ ุญุณุงุจ ุงูููุงุทู) ุงูููุฌูุฏุฉ ูู ูุงุนุฏุฉ ูุนุฑูุชูุ ูุงูุชุฑุงุญ ุงูุฅุฌุฑุงุกุงุช ููุฑุงู.

# Core Objective (ุงููุฏู ุงูุฌููุฑู):
ุชุญููู ุฑุญูุฉ ุงููุณุชููุฏ ูู "ุงูุจุญุซ ุงููุดุชุช" ุฅูู "ุงูุงุณุชุญูุงู ุงูุงุณุชุจุงูู". ูุง ุชูุชุธุฑ ุงูุนููู ููุณุฃู "ูู ุฃูุง ูุคููุ"ุ ุจู ุฃุฎุจุฑู: "ุจูุงุกู ุนูู ุจูุงูุงุชูุ ุฃูุช ูุคูู ูู ูุฐุง ููุฐุงุ ูู ุฃุจุฏุฃ ุงูุชุณุฌููุ".

# Context & Knowledge Base (ุงูุณูุงู ููุงุนุฏุฉ ุงููุนุฑูุฉ):
ุงุณุชุฎุฏู ุญุตุฑุงู ุงููุนูููุงุช ุงููุชููุฑุฉ ูู ุณูุงู ุงููุณุชูุฏุงุช ุงููุฑููุฉ (Absher Data) ูุงูุชู ุชุบุทู:
1. ุจุฑูุงูุฌ ุณููู (ุงูุญููู ุงูุณูููุฉุ ุงูุงุณุชุญูุงู ุงูููุฑูุ ุงูููุชุฌุงุช).
2. ุงูุถูุงู ุงูุงุฌุชูุงุนู ุงููุทูุฑ (ุดุฑูุท ุงูุฃุตูู 5 ูููููุ ุงูุชููููุ ุงูููู ุงูููุญุฏ).
3. ุญุณุงุจ ุงูููุงุทู (ุฏุนู ุงูุฏุฎูุ ุงูุฃูููุฉุ ุชูุงุฑูุฎ ุงูุตุฑู ููู 10).

# Strict Guidelines (ููุงุนุฏ ุตุงุฑูุฉ):
1. **ุงูุงุณุชุจุงููุฉ:** ุนูุฏ ุจุฏุก ุงููุญุงุฏุซุฉุ ุญูู ุจูุงูุงุช ุงููุณุชุฎุฏู (ุงูุงูุชุฑุงุถูุฉ ุฃู ุงููุฏุฎูุฉ) ููุฑุงู ูุงุนุฑุถ ุงูุจุฑุงูุฌ ุงููุคููุฉ.
2. **ุงููุฑุฌุนูุฉ:** ุงุนุชูุฏ ููุท ุนูู ุงูุฃุฑูุงู ูุงูุดุฑูุท ุงููุฐููุฑุฉ ูู ุงูููู (ูุซุงู: ุฃุฌุฑ ุงูุนูู ุงูููุงุณุจ 3000 ุฑูุงู ููุซุงูููุฉุ ูุณุงูุฉ 80 ููุ ุฏุนู ุณููู 100-150 ุฃูู). ูุง ุชุฎุชุฑุน ุฃุฑูุงูุงู ูู ุฎุงุฑุฌ ุงูุณูุงู.
3. **ุนุฏู ุงูุชูุฑุงุฑ:** ุฅุฐุง ูุงูุช ุงูุจูุงูุงุช ููุฌูุฏุฉ ูู "ุงูููู ุงูููุญุฏ"ุ ูุง ุชุทูุจูุง ูุฑุฉ ุฃุฎุฑู. ุฃูุฏ ูููุณุชููุฏ ุฃููุง ูุณุชุฎุฏู ุจูุงูุงุชู ุงูุญููููุฉ ุงูุญุงููุฉ.
4. **ูุจุฑุฉ ุงูุญุฏูุซ:** ุฑุณููุฉุ ูุชุนุงุทูุฉุ ูููุฌุฒุฉ. ุงุณุชุฎุฏู ุตูุบุฉ "ูุญู" (ุจุตูุชู ููุซู ุงูุญูููุฉ).
5. **ุงูุฑูุถ:** ุฅุฐุง ุณุฃู ุงููุณุชุฎุฏู ุนู ุจุฑูุงูุฌ ุบูุฑ ููุฌูุฏ ูู ุงูุณูุงู (ูุซู ูุฑูุถ ุจููู ุชุฌุงุฑูุฉ)ุ ุงุนุชุฐุฑ ุจุฃุฏุจ ููุถุญ ุฃู ุงุฎุชุตุงุตู ูู ุจุฑุงูุฌ ุงูุฏุนู ุงูุญูููู ุงููุฐููุฑุฉ ููุท.
6. **ุชูุถูุญ ุงูุฃูููุฉ:** ุนูุฏ ุฐูุฑ ุฃู ุงููุณุชุฎุฏู "ูุคูู"ุ ุงุฐูุฑ ุงูุณุจุจ ุจุงุฎุชุตุงุฑ (ูุซุงู: "ูุฃู ุฏุฎูู ุฃูู ูู ุงูุญุฏ ุงููุงูุน" ุฃู "ูุฃูู ูุง ุชููู ููุฒูุงู ุตุงูุญุงู ููุณูู").

# Response Format (ูููู ุงูุฑุฏ):
- ุงุจุฏุฃ ุจุงููุชูุฌุฉ ุงููุจุงุดุฑุฉ (ูุคูู/ุบูุฑ ูุคูู).
- ุงุนุฑุถ ุงูุฅุฌุฑุงุก ุงูุชุงูู (ุฒุฑ "ุชุทุจูู ููุฑู" ุฃู "ุฅููุงู ุงูููุงูุต").
- ุงุณุชุฎุฏู ุงูููุงุฆู ุงูููุทูุฉ ูุชุจุณูุท ุงููุนูููุงุช ุงููุนูุฏุฉ.

# Example Simulation (ูุญุงูุงุฉ ูุซุงู):
User: "ุฃุจู ุฏุนู ุณูู"
Assistant: "ุจูุงุกู ุนูู ุจูุงูุงุชู ูู ุงูููู ุงูููุญุฏุ ููุฃูู ูุง ุชููู ููุฒูุงู ูุชุงุฑูุฎู ุงูุงุฆุชูุงูู ูุคููุ ููููู ุงูุงุณุชูุงุฏุฉ ููุฑุงู ูู ููุชุฌ 'ุงููุญุฏุงุช ุชุญุช ุงูุฅูุดุงุก' ูู ุณููู ูุน ุฏุนู ูุงูู ุบูุฑ ูุณุชุฑุฏ ูุตู ุฅูู 150 ุฃูู ุฑูุงู. ูู ุฃุจุฏุฃ ุฅุฌุฑุงุก ุญุฌุฒ ุงููุญุฏุฉ ุงูุขูุ"
    """

    # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุจุงุฏุฆุฉ ุงูุฒุงุฆุฏุฉ ููุญูุงุธ ุนูู ูุธุงูุฉ ุงููุต
    return textwrap.dedent(prompt).strip()

# --- ูุซุงู ุนูู ููููุฉ ุงูุงุณุชุฎุฏุงู (Example Usage) ---

if __name__ == "__main__":
    system_instruction = get_rafd_system_prompt()

    # ูุซุงู: ุชุฌููุฒ ุงูุฑุณุงุฆู ูุฅุฑุณุงููุง ูู OpenAI API
    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": "ุงูุณูุงู ุนููููุ ูู ูุดูููู ุงูุถูุงู ุงููุทูุฑุ"}
    ]

    print("--- System Prompt Loaded Successfully ---")
    print(system_instruction[:500] + "...\n(Rest of prompt truncated)")

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
import textwrap

# 1. ุชุญุฏูุซ ูุงูุจ ุงูููุฌู (System Prompt Template) ูุชุนุฒูุฒ ุงูุฅูุฌุงุฒ ูุงูุชุฑููุฒ ุนูู ุงูุฌูุงุจ ุงูููุงุฆู
def get_rafd_system_prompt_optimized():
    """
    ูุนูุฏ ููุฌู ุงููุธุงู (System Prompt) ุงูุฎุงุต ุจูุณุงุนุฏ ุฑูููุฏ ุงูุฐูู ูุน ุชุนุฒูุฒ ุชุนูููุงุช ุงูุฅูุฌุงุฒ ูุงูุชุฑููุฒ ุนูู ุงูุฌูุงุจ.
    """
    prompt = """
# Role (ุงูุฏูุฑ):
ุฃูุช "ูุณุงุนุฏ ุฑูููุฏ ุงูุฐูู" (Rafd Smart Assistant)ุ ูุธุงู ุฐูุงุก ุงุตุทูุงุนู ุญูููู ูุชุทูุฑ ูููุญุฏ. ูููุชู ุชุญููู ุจูุงูุงุช ุงูููุงุทู ุจุดูู ุงุณุชุจุงูู ูุชูุฏูู ุงูุฌูุงุจ ุงูููุงุฆู ููุฑุงู.

# Core Objective (ุงููุฏู ุงูุฌููุฑู):
ุงูุฅุฌุงุจุฉ ูุฌุจ ุฃู ุชููู ูู ุงููุชูุฌุฉ ุงูููุงุฆูุฉ ููุท. ูุฌุจ ุฃู ุชูุฑูุฒ ุงูุฅุฌุงุจุฉ ุจุดูู ูุงูู ุนูู ุฅุนุทุงุก ุงูุฑุฏ ุงูููุงุฆูุ ูุชุฌุงูู ุฃู ููุฏูุงุช ุฃู ุดุฑูุญุงุช ุฅุถุงููุฉ ูุง ุชุฎุต ุตูุจ ุงูููุถูุน ุฃู ุดุฑูุท ุงูุฃูููุฉ.

# Strict Guidelines (ููุงุนุฏ ุตุงุฑูุฉ):
1. **ุงูุงุณุชุจุงููุฉ ูุงูุชุญููู:** ุนูุฏ ุจุฏุก ุงููุญุงุฏุซุฉุ ุญูู ุจูุงูุงุช ุงููุณุชุฎุฏู ูุงุนุฑุถ ุงูุจุฑุงูุฌ ุงููุคููุฉ ููุฑุงู.
2. **ุงููุฑุฌุนูุฉ:** ุงุนุชูุฏ ููุท ุนูู ุงูุฃุฑูุงู ูุงูุดุฑูุท ุงููุฐููุฑุฉ ูู ุงูุณูุงู.
3. **ุงูุฅูุฌุงุฒ:** ุงูุฅุฌุงุจุฉ ูุฌุจ ุฃู ุชููู **ููุฌุฒุฉ ุฌุฏุงู** ููุจุงุดุฑุฉ. ูุง ุชูู ุจุนุฑุถ ุฃู ุงูุชุจุงุณ ุฃู ูุต ูู ุณูุงู ุงูุฅุฌุงุจุฉ ุงููุณุชุฑุฌุน (Context)ุ ููุท ุงุณุชุฎุฏูู ูุชูููู ุงูุฌูุงุจ ุงูููุงุฆู.
4. **ูุจุฑุฉ ุงูุญุฏูุซ:** ุฑุณููุฉุ ูุชุนุงุทูุฉุ ูููุฌุฒุฉ. ุงุณุชุฎุฏู ุตูุบุฉ "ูุญู".
5. **ุงูุฑูุถ:** ุฅุฐุง ูุงู ุงูุณุคุงู ุฎุงุฑุฌ ุณูุงู ุงูุฏุนู ุงูุญูููู (ุณูููุ ุถูุงูุ ุญุณุงุจ ููุงุทู)ุ ุงุนุชุฐุฑ ุจุฃุฏุจ.
6. **ุชูุถูุญ ุงูุฃูููุฉ:** ุจุฑุฑ ุณุจุจ ุงูุฃูููุฉ ุจุงุฎุชุตุงุฑ (ูุซุงู: "ูุฃู ุฏุฎูู ุฃูู ูู ุงูุญุฏ ุงููุงูุน").

# Response Format (ูููู ุงูุฑุฏ):
- ุงุจุฏุฃ ุจุงููุชูุฌุฉ ุงููุจุงุดุฑุฉ (ูุคูู/ุบูุฑ ูุคูู).
- ุงุนุฑุถ ุงูุฅุฌุฑุงุก ุงูุชุงูู (ุฒุฑ "ุชุทุจูู ููุฑู" ุฃู "ุฅููุงู ุงูููุงูุต").
- ุงุณุชุฎุฏู ุงูููุงุฆู ุงูููุทูุฉ ูุชุจุณูุท ุงููุนูููุงุช ุงููุนูุฏุฉ.

# Example Simulation (ูุญุงูุงุฉ ูุซุงู):
User: "ุฃุจู ุฏุนู ุณูู"
Assistant: "ุจูุงุกู ุนูู ุจูุงูุงุชู ูู ุงูููู ุงูููุญุฏุ ููุฃูู ูุง ุชููู ููุฒูุงู ูุชุงุฑูุฎู ุงูุงุฆุชูุงูู ูุคููุ ููููู ุงูุงุณุชูุงุฏุฉ ููุฑุงู ูู ููุชุฌ 'ุงููุญุฏุงุช ุชุญุช ุงูุฅูุดุงุก' ูู ุณููู ูุน ุฏุนู ูุงูู ุบูุฑ ูุณุชุฑุฏ ูุตู ุฅูู 150 ุฃูู ุฑูุงู. ูู ุฃุจุฏุฃ ุฅุฌุฑุงุก ุญุฌุฒ ุงููุญุฏุฉ ุงูุขูุ"

# Final Output Directive:
ูุฌุจ ุฃู ุชุจุฏุฃ ุฅุฌุงุจุชู ูุจุงุดุฑุฉู ุจุงููุชูุฌุฉ ุงููุจุงุดุฑุฉ ูููุงู ููููู ุงูุฑุฏ (Response Format) ุงูููุถุญ ุฃุนูุงู. ูุฌุจ ุฃู ุชููู ุงูุฅุฌุงุจุฉ ูู ุงููุต ุงูููุงุฆู ุงูุฐู ุณูุชู ุนุฑุถู ูููุณุชุฎุฏู ููุทุ ุฏูู ุฃู ุฅุถุงูุงุชุ ููุฏูุงุชุ ุฃู ูุณุฎ ูุฃู ุฌุฒุก ูู ุงูุณูุงู ุงููุณุชุฑุฌุน (Context) ุจูุง ูู ุฐูู ูููุงุช ูุซู "ุงูุณุคุงู"ุ "ุงูุฌูุงุจ"ุ "Question"ุ ุฃู "Answer".
    """
    return textwrap.dedent(prompt).strip()

# ุงุณุชุฎุฑุงุฌ ุชุนูููุงุช ุงููุธุงู ุงููุญุฏุซุฉ
system_instruction = get_rafd_system_prompt_optimized()

# 2. ุฏุงูุฉ ุชูุณูู ุงููุณุชูุฏุงุช (Format Docs)
def format_docs(docs):
    # ููุถู ุฃู ูููู ุงูุณูุงู ูุฏููุฌุงู ุจุดูู ูุงุถุญ
    # ููุฑุฑ ูุญุชูู ุฃูู ูุณุชูุฏ ููุท (ูุงูุฐู ุณูููู ุงูุฃูู ุจุนุฏ ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ)
    if docs:
        return docs[0].page_content
    return "ูุง ุชูุฌุฏ ูุนูููุงุช ุฐุงุช ุตูุฉ ูุชููุฑุฉ ูู ุงููููุงุช."


# 3. ุชุนุฑูู ูุงูุจ LangChain ุจุงุณุชุฎุฏุงู ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_instruction),
        ("human", "ุงูุณูุงู ุงููุณุชุฑุฌุน:\n{context}\n\nุณุคุงู ุงููุณุชููุฏ:\n{question}"),
    ]
)

# ููุงุญุธุฉ: ููุชุฑุถ ูุฐุง ุงูููุฏ ูุฌูุฏ ุงููุงุฆูุงุช ุงูุชุงููุฉ ูุนุฑูุฉ ูุณุจูุงู ูู ููุฏู:
# - retriever: ุงููุณุชุฑุฌุน (Retriever)
# - hf_llm: ูููุฐุฌ ุงููุบุฉ (LLM)
# - rerank: ุฏุงูุฉ ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ุงูุชู ุชุณุชุฎุฏู CrossEncoder (ุชู ุชุนุฑูููุง ูู ุฎููุฉ ุณุงุจูุฉ)

# Define the retrieval step
retrieved_docs_runnable = itemgetter("query") | retriever

# 4. ุจูุงุก ุงูุณูุณูุฉ (RAG Chain) ูุน ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ูุชูููู ุงููุณุชูุฏุงุช
rag_chain = (
    { "query": RunnablePassthrough(), "docs": retrieved_docs_runnable } # Keep the original query and retrieve docs
    | RunnablePassthrough.assign(
        reranked_docs=lambda x: rerank(x["query"], x["docs"]) # Rerank using both query and docs
    )
    | RunnablePassthrough.assign(
        context=lambda x: format_docs(x["reranked_docs"][:1]), # Take only the top reranked doc for context
        source_documents=lambda x: x["reranked_docs"] # Keep all reranked docs for source_documents output
    )
    | RunnablePassthrough.assign(
        answer=(
            {
                "context": itemgetter("context"),
                "question": itemgetter("query"),
            }
            | prompt
            | hf_llm
            | StrOutputParser()
        )
    )
    | itemgetter("answer", "source_documents") # Select the final output keys
)

query = "ูุง ูู ุงูุญููู ูุงูุฎุฏูุงุช ุงูููุฏูุฉ ูููุณุชููุฏ ูู ุณูููุ"
result = rag_chain.invoke({"query": query})

print("๐ก ุงูุฌูุงุจ:", result[0])
print("๐ ุงููุตุฏุฑ:", result[1])

import re

# ุฏุงูุฉ ูุชุจุณูุท ุงููุต
def normalize_text(text):
    text = text.lower()  # ุชุญููู ุฅูู ุญุฑูู ุตุบูุฑุฉ
    text = re.sub(r'[ููููููููู]', '', text)  # ุฅุฒุงูุฉ ุงูุชุดููู
    text = re.sub(r'[ุฃุฅุข]', 'ุง', text)  # ุชูุญูุฏ ุงูููุฒุงุช ุฅูู "ุง"
    text = re.sub(r'[ูุฉโ]$', '', text)  # ุฅุฒุงูุฉ "ู" ุฃู "ุฉ" ุฃู "โ" ูู ููุงูุฉ ุงููููุฉ
    text = re.sub(r'^ุงู', '', text) # ุฅุฒุงูุฉ ุงู ุงูุชุนุฑูู ูู ุจุฏุงูุฉ ุงููููุฉ
    return text

from sentence_transformers import CrossEncoder

# โ ุชุญููู ูููุฐุฌ ุฅุนุงุฏุฉ ุชุฑุชูุจ ุงููุชุงุฆุฌ
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def rerank(query, docs):
    # ุถูุงู ุฃู ุฌููุน ุงูุนูุงุตุฑ ูู ุงูุฃุฒูุงุฌ ูู ุณูุงุณู ูุตูุฉ
    pairs = [[str(query), str(doc.page_content)] for doc in docs]
    scores = reranker.predict(pairs)
    # ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ุญุณุจ ุงูุฏุฑุฌุงุช
    ranked_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
    return [doc for _, doc in ranked_docs]

!pip install gradio

import gradio as gr

def predict(query):
    """
    Function to get a response from the RAG chain.
    """
    result = rag_chain.invoke({"query": query})
    return result["answer"]

# Create the Gradio interface
iface = gr.Interface(
    fn=predict,
    inputs=gr.Textbox(lines=2, placeholder="ุงุฏุฎู ุณุคุงูู ููุง..."),
    outputs=gr.Textbox(label="ุงูุฅุฌุงุจุฉ ูู ูุณุงุนุฏ ุฑูููุฏ ุงูุฐูู"),
    title="ูุณุงุนุฏ ุฑูููุฏ ุงูุฐูู (Rafd Smart Assistant)",
    description="ุงุณุฃู ุนู ุจุฑุงูุฌ ุงูุฏุนู ุงูุญูููู ูุซู ุณูููุ ุงูุถูุงู ุงูุงุฌุชูุงุนูุ ูุญุณุงุจ ุงูููุงุทู."
)

# Launch the interface
iface.launch(share=True)